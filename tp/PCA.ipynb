{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "678df900",
   "metadata": {},
   "source": [
    "# TE62MI - Algebre linéaire et réduction de dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c203280e",
   "metadata": {},
   "source": [
    "## TP 2. Analyse en Composantes Principales (ACP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111088e2",
   "metadata": {},
   "source": [
    "### Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4c2582",
   "metadata": {},
   "source": [
    "L'objectif de ce TP est d'étudier et mettre en œuvre l'analyse en composantes principales (ACP) comme méthode de réduction de dimensionnalité, sur des données réelles. Nous appliquerons l'ACP sur un jeu de données de vins Italiens, qui correspond à l'analyse chimique de vins cultivés dans la même région en Italie mais issus de trois cépages différents (trois types de vin; trois classes). Ce jeu de données est composé de $n=178$ observations, décrivant les quantités de $d=13$ constituants trouvés dans chacun d'eux. Initialement, nous divisons l'ensemble des données en un sous ensemble d'entraînement et un sous ensemble de test (après avoir mélangé les données pour avoir des instances de toutes les classes dans chaque sous ensemble)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ffec3e",
   "metadata": {},
   "source": [
    "### Rappels de cours\n",
    "\n",
    "Pour un ensemble de points en dimensions $d$, l'ACP vise à trouver un sous-espace linéaire de dimension $k \\leq d$ tel que les points se trouvent principalement sur ce sous-espace. Un tel sous-espace tente de maintenir la majeure partie de la variance des données et peut être spécifié par $d$ vecteurs orthogonaux qui forment un nouveau système de coordonnées, appelé les composantes principales. L'espoir est que seul $ k << d$ composantes suffisent pour approximer l'espace couvert par les $d$\n",
    "axes originaux. \n",
    "\n",
    "Ainsi, la première composante principale maximise la variance après projection (c'est-à-dire qu'elle représente autant de variabilité des données que possible), et chaque composante successive a à son tour la variance la plus élevée possible sous la contrainte d'être orthogonal (c'est-à-dire non corrélé) avec les composantes précédentes.\n",
    "\n",
    "L'ACP peut être calculée en utilisant la décomposition en valeurs propres de la matrice de covariance comme\n",
    "suit :\n",
    "1. Soit $X \\in \\mathbb{R}^{nd}$ une matrice correspondant à $n$ points en dimension $d$. Centrer $X$ en soustrayant les valeurs moyennes des\n",
    "colonnes : $X = X - M$\n",
    "2. Calculer la matrice de covariance $C= X^T X$\n",
    "3. Trouver les valeurs propres et les vecteurs propres $V$ de $C$\n",
    "4. Projeter les données $X$ sur les $k$ vecteurs propres $V_{[1:k]}$ correspondant aux $k$ plus grandes valeurs propres : $X' = XV_{[1:k]}^T \\in \\mathbb{R}^{nk}$\n",
    "\n",
    "La fraction de variance préservée dans les données transformées/projetées est donnée par :\n",
    "$var = \\frac{\\sum_{i=0}^k \\lambda_i}{\\sum_{j=0}^d \\lambda_j}$\n",
    "où les $\\lambda_j$ sont les valeurs propres de la matrice de covariance triées par ordre décroissant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a46c4ba",
   "metadata": {},
   "source": [
    "### Tâches à effectuer dans ce TP\n",
    "\n",
    "Compléter les champs de code manquant, entre `### TODO ###` et `### ENDO ###` afin de\n",
    "mettre en œuvre la technique de l'ACP\n",
    "- Compléter les méthodes `fit` et `transform` dans la classe appelée `PCA` (`section 2`) \n",
    "- Tester votre implémentation et comparer à scikit-learn. Inspecter la distribution des valeurs singulières de $X$ (`section 3`) \n",
    "- Quelle est la variance expliquée par chaque composante principale? (`section 3`)\n",
    "- Projeter les données en 3D (sur les 3 premières composantes) et les visualiser en 2D. (`section 4`)\n",
    "- Qu'observez-vous ? Quel cépage a la plus grande variance ? (`section 4`)\n",
    "- Interpréter le sens des vecteurs propres appris. Quelles caractéristiques d'origine affectent l'espace de dimension réduite? (`section 5`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb1a087",
   "metadata": {},
   "source": [
    "## 1. Lire le jeu de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db80d6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data (Wine dataset)\n",
    "import numpy as np\n",
    "dataset = np.genfromtxt('data/wine_data.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df28ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle and split datataset\n",
    "np.random.seed(42) # seed to reproduce results\n",
    "np.random.shuffle(dataset)\n",
    "X_train = dataset[:100,1:] # training data\n",
    "X_test = dataset[101:,1:] # testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e717bb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print X_train shape (n,d)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bd32dc",
   "metadata": {},
   "source": [
    "## 2. Implémentation de l'ACP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31a547b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCA(object):\n",
    "    \n",
    "    def __init__(self, n_components=2):\n",
    "        \n",
    "        # input space\n",
    "        self.n_samples_ = None # a number, int (n)\n",
    "        self.n_features_in_  = None # a number, int (d)\n",
    "        \n",
    "        # center data\n",
    "        self.mean_ = None # a vector, ndarray of shape (d,)\n",
    "        \n",
    "        # principal components\n",
    "        self.n_components_ = n_components # a number, int (k)\n",
    "        self.components_ = None # a matrix, ndarray of shape (k, d), aka eigenvectors\n",
    "        self.singular_values_ = None # a vector, ndarray of shape (k,), aka eigenvalues**1/2\n",
    "        self.explained_variance_ratio_ = None #  a vector, ndarray of shape (k,)\n",
    "    \n",
    "    def fit(self, X):\n",
    "        ''' \n",
    "        Performs principal components analysis (PCA) on the n-by-d data matrix X (data)\n",
    "        Rows of X correspond to observations (wines), columns to variables.\n",
    "        \n",
    "        Saves values for self.n_features_in_, self.n_samples_, self.mean_\n",
    "        as well as self.singular_values_, self.components_ and self.explained_variance_ratio_\n",
    "        For example, self.n_features_in_ = X.shape[1]\n",
    "        \n",
    "        Args:\n",
    "            X: the data matrix (n,p)\n",
    "        '''\n",
    "        \n",
    "        k = self.n_components_\n",
    "        self.n_samples_ = X.shape[0]\n",
    "        self.n_features_in_ = X.shape[1]\n",
    "        \n",
    "        # Compute the mean and center X (save self.mean_)\n",
    "\n",
    "        ### TODO ###\n",
    "        #\n",
    "        #\n",
    "        ### ENDO ###\n",
    "        \n",
    "        # Calculate the covariance matrix C\n",
    "        \n",
    "        ### TODO ###\n",
    "        #\n",
    "        #\n",
    "        ### ENDO ###\n",
    "\n",
    "        # Save the matrix decomposition parameters (save self.singular_values_, self.components_)\n",
    "        \n",
    "        ### TODO ###\n",
    "        #\n",
    "        #\n",
    "        #\n",
    "        ## ENDO ##\n",
    "        \n",
    "        # Measure the quality of the low rank approximation (save self.explained_variance_ratio_)\n",
    "        \n",
    "        ### TODO ###\n",
    "        #\n",
    "        ## ENDO ##\n",
    "        \n",
    "    def transform(self, X):\n",
    "        ''' \n",
    "        Reduces dimensonality of the n-by-d data matrix X using principal components analysis (PCA)\n",
    "        \n",
    "        Args:\n",
    "            X: the data matrix (n,d)\n",
    "        Returns:\n",
    "            y: the data in a lower dimension (n, k)\n",
    "        '''\n",
    "        \n",
    "        # Project X on the new space\n",
    "        \n",
    "        ## TODO ##\n",
    "        #\n",
    "        #\n",
    "        #\n",
    "        ## ENDO ##\n",
    "    \n",
    "    def fit_transform(self, X):\n",
    "        self.fit(X)\n",
    "        return self.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b64bda",
   "metadata": {},
   "source": [
    "## 3. Tester l'implémentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a68c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install scikit-learn # uncomment this line to install scikit learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabfcfa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.decomposition import PCA # uncomment this line to compare your implementation to scikit learn\n",
    "pca = PCA(n_components=3)\n",
    "pca.fit(X_train)\n",
    "\n",
    "assert pca.n_samples_ == X_train.shape[0]\n",
    "assert pca.n_features_in_ == X_train.shape[1]\n",
    "assert pca.n_components_ == 3\n",
    "assert pca.mean_.shape == (X_train.shape[1],)\n",
    "assert pca.components_.shape == (3, X_train.shape[1])\n",
    "assert pca.singular_values_.shape == (3,)\n",
    "assert np.sum(np.abs(pca.singular_values_)-pca.singular_values_)==0 # positive values\n",
    "assert pca.explained_variance_ratio_[0] >= 0.\n",
    "\n",
    "print(pca.singular_values_)\n",
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97141971",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = pca.transform(X_train)\n",
    "y_test = pca.transform(X_test)\n",
    "\n",
    "assert y_train.shape == (X_train.shape[0], 3)\n",
    "assert y_test.shape == (X_test.shape[0], 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f549980a",
   "metadata": {},
   "source": [
    "## 4. Visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b46d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_data(offset=0):\n",
    "    plt.figure(1, figsize=(10,5))\n",
    "\n",
    "    plt.subplot(121)\n",
    "    c_train = dataset[:100,0] # class labels of training data (cultivars, type of wine) \n",
    "    plt.scatter(y_train[:,offset], y_train[:,offset+1], c=c_train)\n",
    "    plt.xlabel(\"Principal component {}\".format(offset+1))\n",
    "    plt.ylabel(\"Principal component {}\".format(offset+2))\n",
    "    plt.title(\"PCA on the WINE dataset (train)\")\n",
    "\n",
    "    plt.subplot(122)\n",
    "    c_test = dataset[101:,0] # class labels of testing data (cultivars, type of wine) \n",
    "    plt.scatter(y_test[:,offset], y_test[:,offset+1], c=c_test)\n",
    "    plt.xlabel(\"Principal component {}\".format(offset+1))\n",
    "    plt.ylabel(\"Principal component {}\".format(offset+2))\n",
    "    plt.title(\"PCA on the WINE dataset (test)\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505c4da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_data(offset=0) # change to 1 to visualize projection on principal components 2 and 3\n",
    "\n",
    "### TODO ###\n",
    "# Qu'observez vous?\n",
    "# Quelles composantes principales permettent de distinguer les trois types de vins?\n",
    "### ENDO ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee82da0a",
   "metadata": {},
   "source": [
    "## 5. Analyse model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232f7bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO ###\n",
    "# À quelles dimensions de l'espace d'origine correspondent les composantes principales?\n",
    "### ENDO ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72e8025",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(np.abs(pca.components_[0]), np.arange(len(pca.components_[0])), label=\"PC1\", alpha=0.5)\n",
    "plt.scatter(np.abs(pca.components_[1]), np.arange(len(pca.components_[1])), label=\"PC2\", alpha=0.5)\n",
    "plt.scatter(np.abs(pca.components_[2]), np.arange(len(pca.components_[2])), label=\"PC3\", alpha=0.5)\n",
    "plt.xlabel(\"|coefficient value|\")\n",
    "plt.xlim(-0.1,1.1)\n",
    "plt.ylabel(\"Features (original space)\")\n",
    "plt.title(\"PC1 and PC3 coordinates\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ec3197",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
