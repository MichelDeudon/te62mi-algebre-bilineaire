{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "678df900",
   "metadata": {},
   "source": [
    "# TE62MI - Algebre linéaire et réduction de dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c203280e",
   "metadata": {},
   "source": [
    "## TP 2. Analyse en Composantes Principales (ACP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111088e2",
   "metadata": {},
   "source": [
    "### Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4c2582",
   "metadata": {},
   "source": [
    "L'objectif de ce TP est d'étudier et mettre en œuvre l'analyse en composantes principales (ACP) comme méthode de réduction de dimensionnalité, sur des données réelles. Nous appliquerons l'ACP sur un jeu de données de vins Italiens, qui correspond à l'analyse chimique de vins cultivés dans la même région en Italie mais issus de trois cépages différents (trois types de vin, varités; trois classes). Ce jeu de données est composé de $n=178$ observations, décrivant les quantités de $d=13$ constituants trouvé dans chacun d'eux. Ainsi, la taille de l'ensemble de données est de 178 x 13. Initialement, nous chargeons les données et divisons l'ensemble de données en un sous ensemble d'entraînement et de test (après avoir mélangé les données pour avoir des instances de toutes les classes dans chaque sous ensemble)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ffec3e",
   "metadata": {},
   "source": [
    "### ACP (analyse en composantes principales)\n",
    "\n",
    "Pour un ensemble de données de dimensions $d$, l'ACP vise à trouver un sous-espace linéaire de dimension $k$ inférieur à $d$ tel que les points de données se trouvent principalement sur ce sous-espace linéaire. Un tel sous-espace tente de maintenir la majeure partie de la variance des données. Le sous-espace linéaire peut être spécifié par $d$ vecteurs orthogonaux qui forment un nouveau système de coordonnées, appelé les composantes principales (il ne peut y en avoir plus de $n$). L'espoir est que seuls $dk< d$ composants principaux suffisent pour approximer l'espace couvert par les $d$\n",
    "axes originaux. Ainsi, pour un ensemble donné de vecteurs de données $x_i; i \\in 1... n$, les axes principaux $k$ sont les axes orthonormés sur lequel la variance retenue par projection est maximale. Cette transformation est définie dans une telle manière à ce que la première composante principale ait la plus grande variance possible (c'est-à-dire qu'elle représente autant de variabilité des données que possible), et chaque composante successive a à son tour la variance la plus élevée possible sous la contrainte qu'être orthogonal (c'est-à-dire non corrélé) avec les composants précédents.\n",
    "Rappelons que la variance d'une variable aléatoire est donnée par la formule suivante : $\\Sigma^2 (X) = \\sigma^2 =\n",
    "\\mathbb{E}[(X -\\mu)^2]$. L'ACP peut être calculée en utilisant la décomposition des valeurs propres de la matrice de covariance comme\n",
    "suit :\n",
    "1. Supposons que les données sont organisées dans une matrice $mxn$ $X$, soustrayez initialement les valeurs moyennes de\n",
    "colonnes : $X = X - M$\n",
    "2. Calculer la matrice de covariance $C= X^T X$\n",
    "3. Trouver les valeurs propres et les vecteurs propres de la matrice de covariance $C$\n",
    "4. Composantes principales : les $k$ vecteurs propres $V_{[1:k]}$ qui correspondent aux $k$ plus grandes valeurs propres\n",
    "5. Projetez les données dans le nouvel espace : $XV_{[1:k]}$\n",
    "\n",
    "Comme nous l'avons déjà discuté, l'ACP transforme les données dans un espace de dimension inférieure en préservant\n",
    "la variance. La fraction de variance qui est préservée dans les données transformées peut être capturée par le\n",
    "formule suivante :\n",
    "\n",
    "\\begin{equation}\n",
    "    var = \\frac{\\sum_{i=0}^k \\lambda_i}{\\sum_{j=0}^d \\lambda_j}\n",
    "\\end{equation}\n",
    "\n",
    "où $d$ est le nombre de dimensions dans l'ensemble de données d'origine, $k$ est le nombre de dimensions dans le nouveau\n",
    "l'espace de représentation, et $\\lambda$ sont les valeurs propres de la matrice de covariance triées par ordre décroissant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a46c4ba",
   "metadata": {},
   "source": [
    "### Tâches à effectuer dans ce TP\n",
    "\n",
    "Compléter les champs de code manquant, entre `### TODO ###` et `### ENDO ###` afin de\n",
    "mettre en œuvre la technique de l'ACP\n",
    "- Compléter les méthodes `fit` et `transform` dans la classe appelée `PCA` (`section 2`) \n",
    "- Tester votre implémentation et comparer à sciki-learn. Inspecter la distribution des valeurs singulières de $X$ (`section 3`) \n",
    "- Quelle est la variance expliquée par chaque composante principale? (`section 3`)\n",
    "- Projeter les données en 3D (sur les 3 premières composantes) et les visualiser en 2D. (`section 4`)\n",
    "- Qu'observez-vous ? Pouvez-vous trouver le cépage avec la plus grande « variation » ? (`section 4`)\n",
    "- Interpréter le sens des vecteurs propres appris. Quelles caractéristiques d'origine affectent l'espace de dimension réduite? (`section 5`) \n",
    "- Tester avec un autre dataset de votre choix (`section 1`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb1a087",
   "metadata": {},
   "source": [
    "## 1. Load the Wine dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db80d6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data (Wine dataset)\n",
    "import numpy as np\n",
    "dataset = np.genfromtxt('data/wine_data.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df28ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle and split datataset\n",
    "np.random.seed(42) # seed to reproduce results\n",
    "np.random.shuffle(dataset)\n",
    "X_train = dataset[:100,1:] # training data\n",
    "X_test = dataset[101:,1:] # testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e717bb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print X_train shape (n,d)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bd32dc",
   "metadata": {},
   "source": [
    "## 2. Implement PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31a547b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCA(object):\n",
    "    \n",
    "    def __init__(self, n_components=2):\n",
    "        \n",
    "        # input space\n",
    "        self.n_features_in_  = None # a number, int (d)\n",
    "        self.n_samples_ = None # a number, int (n)\n",
    "        \n",
    "        # center data\n",
    "        self.mean_ = None # a vector, ndarray of shape (d,)\n",
    "        \n",
    "        # principal components\n",
    "        self.n_components_ = n_components # a number, int (k)\n",
    "        self.components_ = None # a matrix, ndarray of shape (k, d), aka eigenvectors\n",
    "        self.singular_values_ = None # a vector, ndarray of shape (k,), aka eigenvalues**1/2\n",
    "        self.explained_variance_ratio_ = None #  a vector, ndarray of shape (k,)\n",
    "    \n",
    "    def fit(self, X):\n",
    "        ''' \n",
    "        Performs principal components analysis (PCA) on the n-by-d data matrix X (data)\n",
    "        Rows of X correspond to observations (wines), columns to variables.\n",
    "        \n",
    "        Saves values for self.n_features_in_, self.n_samples_, self.mean_\n",
    "        as well as self.singular_values_, self.components_ and self.explained_variance_ratio_\n",
    "        For example, self.n_features_in_ = X.shape[1]\n",
    "        \n",
    "        Args:\n",
    "            X: the data matrix (n,p)\n",
    "        '''\n",
    "        \n",
    "        k = self.n_components_\n",
    "        self.n_samples_ = X.shape[0]\n",
    "        self.n_features_in_ = X.shape[1]\n",
    "        \n",
    "        # Compute the mean and center X (save self.mean_)\n",
    "\n",
    "        ### TODO ###\n",
    "        #\n",
    "        #\n",
    "        ### ENDO ###\n",
    "        \n",
    "        # Calculate the covariance matrix C\n",
    "        \n",
    "        ### TODO ###\n",
    "        #\n",
    "        #\n",
    "        ### ENDO ###\n",
    "\n",
    "        # Save the matrix decomposition parameters (save self.singular_values_, self.components_)\n",
    "        \n",
    "        ### TODO ###\n",
    "        #\n",
    "        #\n",
    "        #\n",
    "        ## ENDO ##\n",
    "        \n",
    "        # Measure the quality of the low rank approximation (save self.explained_variance_ratio_)\n",
    "        \n",
    "        ### TODO ###\n",
    "        #\n",
    "        ## ENDO ##\n",
    "        \n",
    "    def transform(self, X):\n",
    "        ''' \n",
    "        Reduces dimensonality of the n-by-d data matrix X using principal components analysis (PCA)\n",
    "        \n",
    "        Args:\n",
    "            X: the data matrix (n,d)\n",
    "        Returns:\n",
    "            y: the data in a lower dimension (n, k)\n",
    "        '''\n",
    "        \n",
    "        # Project X on the new space\n",
    "        \n",
    "        ## TODO ##\n",
    "        #\n",
    "        #\n",
    "        #\n",
    "        ## ENDO ##\n",
    "    \n",
    "    def fit_transform(self, X):\n",
    "        self.fit(X)\n",
    "        return self.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b64bda",
   "metadata": {},
   "source": [
    "## 3. Test your implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a68c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install scikit-learn # uncomment this line to install scikit learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabfcfa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.decomposition import PCA # uncomment this line to compare your implementation to scikit learn\n",
    "pca = PCA(n_components=3)\n",
    "pca.fit(X_train)\n",
    "\n",
    "assert pca.n_samples_ == X_train.shape[0]\n",
    "assert pca.n_features_in_ == X_train.shape[1]\n",
    "assert pca.n_components_ == 3\n",
    "assert pca.mean_.shape == (X_train.shape[1],)\n",
    "assert pca.components_.shape == (3, X_train.shape[1])\n",
    "assert pca.singular_values_.shape == (3,)\n",
    "assert np.sum(np.abs(pca.singular_values_)-pca.singular_values_)==0 # positive values\n",
    "assert pca.explained_variance_ratio_[0] >= 0.\n",
    "\n",
    "print(pca.singular_values_)\n",
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97141971",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = pca.transform(X_train)\n",
    "y_test = pca.transform(X_test)\n",
    "\n",
    "assert y_train.shape == (X_train.shape[0], 3)\n",
    "assert y_test.shape == (X_test.shape[0], 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f549980a",
   "metadata": {},
   "source": [
    "## 4. Visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b46d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_data(offset=0):\n",
    "    plt.figure(1, figsize=(10,5))\n",
    "\n",
    "    plt.subplot(121)\n",
    "    c_train = dataset[:100,0] # class labels of training data (cultivars, type of wine) \n",
    "    plt.scatter(y_train[:,offset], y_train[:,offset+1], c=c_train)\n",
    "    plt.xlabel(\"Principal component {}\".format(offset+1))\n",
    "    plt.ylabel(\"Principal component {}\".format(offset+2))\n",
    "    plt.title(\"PCA on the WINE dataset (train)\")\n",
    "\n",
    "    plt.subplot(122)\n",
    "    c_test = dataset[101:,0] # class labels of testing data (cultivars, type of wine) \n",
    "    plt.scatter(y_test[:,offset], y_test[:,offset+1], c=c_test)\n",
    "    plt.xlabel(\"Principal component {}\".format(offset+1))\n",
    "    plt.ylabel(\"Principal component {}\".format(offset+2))\n",
    "    plt.title(\"PCA on the WINE dataset (test)\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505c4da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_data(offset=0) # change to 1 to visualize projection on principal components 2 and 3\n",
    "\n",
    "### TODO ###\n",
    "# What do you observe?\n",
    "# Which principal components help distinguish types of wine? [PC1, PC3]\n",
    "### ENDO ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee82da0a",
   "metadata": {},
   "source": [
    "## 5. Analyse model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232f7bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO ###\n",
    "# What dimensions of the original space do the principal components encode?\n",
    "### ENDO ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72e8025",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(np.abs(pca.components_[0]), np.arange(len(pca.components_[0])), label=\"PC1\", alpha=0.5)\n",
    "plt.scatter(np.abs(pca.components_[1]), np.arange(len(pca.components_[1])), label=\"PC2\", alpha=0.5)\n",
    "plt.scatter(np.abs(pca.components_[2]), np.arange(len(pca.components_[2])), label=\"PC3\", alpha=0.5)\n",
    "plt.xlabel(\"|coefficient value|\")\n",
    "plt.xlim(-0.1,1.1)\n",
    "plt.ylabel(\"Features (original space)\")\n",
    "plt.title(\"PC1 and PC3 coordinates\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ec3197",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
